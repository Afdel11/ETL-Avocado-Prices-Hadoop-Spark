# ü•ë Pipeline Big Data pour l'Analyse des March√©s d'Avocats

**Une solution compl√®te de traitement de donn√©es pour l'analyse du march√© des avocats utilisant l'√©cosyst√®me Hadoop**

![Architecture du Pipeline](media/pipeline_arch.png) *Diagramme d'architecture exemple*

## üìå Aper√ßu du Projet
Ce projet impl√©mente une solution Big Data compl√®te pour traiter et analyser les donn√©es de march√© des avocats depuis des fichiers CSV jusqu'√† des insights actionnables. Le pipeline int√®gre :
- **MySQL** pour le stockage initial des donn√©es
- **Apache NiFi** pour l'automatisation de l'ingestion
- **HDFS** pour le stockage distribu√©
- **Spark** pour la transformation des donn√©es
- **Hive** pour les requ√™tes de type SQL
- **Cron** pour l'orchestration du workflow

## üõ†Ô∏è Stack Technique
| Composant       | Technologie Utilis√©e |
|-----------------|----------------------|
| Ingestion       | Apache NiFi          |
| Stockage        | MySQL, HDFS          |
| Traitement      | PySpark              |
| Data Warehouse  | Hive                 |
| Orchestration   | Cron                 |

## üìÇ √âtapes du Pipeline
1. **Pr√©paration des Donn√©es**
   - D√©coupage du CSV source en partitions
   - Chargement dans la base MySQL

2. **Ingestion Automatis√©e**
   - Workflow NiFi extrait les donn√©es de MySQL
   - Stockage des fichiers bruts dans HDFS (`/raw_avocado`)

3. **Traitement des Donn√©es**
   - Job Spark #1 : Calcule les m√©triques de volume ‚Üí Table Hive
   - Job Spark #2 : Enrichit avec des features de date ‚Üí Stockage HDFS raffin√© (`/refine_avocado`)

4. **Analyse**
   - Tables externes Hive pour acc√®s SQL
   - Vues agr√©g√©es (ex : `volume_per_month`)

5. **Automatisation**
   - NiFi programm√© toutes les 5 minutes
   - Jobs Spark planifi√©s via Cron

## üöÄ Guide de D√©marrage
### Pr√©requis
- Cluster Hadoop
- MySQL 8.0+
- Apache NiFi
- Spark 3.x
- Hive 3.x

### Installation

# Cloner le d√©p√¥t
git clone [https://github.com/Afdel11/ETL-Avocado-Prices-Hadoop-Spark.git]

# Cr√©er les r√©pertoires HDFS
hdfs dfs -mkdir /raw_avocado
hdfs dfs -mkdir /staging_avocado
hdfs dfs -mkdir /refine_avocado

## üìù Fonctionnalit√©s Cl√©s
### Workflow Automatis√© : Pipeline de donn√©es enti√®rement planifi√©

- Qualit√© des Donn√©es : Validation √† chaque √©tape

- Scalable : Traitement distribu√© des gros volumes

- Reproductible : Composants dockeris√©s disponibles

## üìà R√©sultats
### Traitement de 5+ millions d'enregistrements

- 99.9% de coh√©rence des donn√©es

- R√©duction du temps de traitement de plusieurs heures √† quelques minutes

## ü§ù Contributeurs
- Afdel Desmond KOMBOU

- Papa Yeriba NIANG

- Supervis√© par Mr. Patrick NGOUNE

