# ğŸ¥‘ Pipeline Big Data pour l'Analyse des MarchÃ©s d'Avocats

**Une solution complÃ¨te de traitement de donnÃ©es pour l'analyse du marchÃ© des avocats utilisant l'Ã©cosystÃ¨me Hadoop**

![Architecture du Pipeline](media/pipeline_arch.png) *Diagramme d'architecture exemple*

## ğŸ“Œ AperÃ§u du Projet
Ce projet implÃ©mente une solution Big Data complÃ¨te pour traiter et analyser les donnÃ©es de marchÃ© des avocats depuis des fichiers CSV jusqu'Ã  des insights actionnables. Le pipeline intÃ¨gre :
- **MySQL** pour le stockage initial des donnÃ©es
- **Apache NiFi** pour l'automatisation de l'ingestion
- **HDFS** pour le stockage distribuÃ©
- **Spark** pour la transformation des donnÃ©es
- **Hive** pour les requÃªtes de type SQL
- **Cron** pour l'orchestration du workflow

## ğŸ› ï¸ Stack Technique
| Composant       | Technologie UtilisÃ©e |
|-----------------|----------------------|
| Ingestion       | Apache NiFi          |
| Stockage        | MySQL, HDFS          |
| Traitement      | PySpark              |
| Data Warehouse  | Hive                 |
| Orchestration   | Cron                 |

## ğŸ“‚ Ã‰tapes du Pipeline
1. **PrÃ©paration des DonnÃ©es**
   - DÃ©coupage du CSV source en partitions
   - Chargement dans la base MySQL

2. **Ingestion AutomatisÃ©e**
   - Workflow NiFi extrait les donnÃ©es de MySQL
   - Stockage des fichiers bruts dans HDFS (`/raw_avocado`)

3. **Traitement des DonnÃ©es**
   - Job Spark #1 : Calcule les mÃ©triques de volume â†’ Table Hive
   - Job Spark #2 : Enrichit avec des features de date â†’ Stockage HDFS raffinÃ© (`/refine_avocado`)

4. **Analyse**
   - Tables externes Hive pour accÃ¨s SQL
   - Vues agrÃ©gÃ©es (ex : `volume_per_month`)

5. **Automatisation**
   - NiFi programmÃ© toutes les 5 minutes
   - Jobs Spark planifiÃ©s via Cron

## ğŸš€ Guide de DÃ©marrage
### PrÃ©requis
- Cluster Hadoop
- MySQL 8.0+
- Apache NiFi
- Spark 3.x
- Hive 3.x

### Installation

# Cloner le dÃ©pÃ´t
git clone https://github.com/votre-repo/avocado-bigdata.git

# CrÃ©er les rÃ©pertoires HDFS
hdfs dfs -mkdir /raw_avocado
hdfs dfs -mkdir /staging_avocado
hdfs dfs -mkdir /refine_avocado

##ğŸ“ FonctionnalitÃ©s ClÃ©s
###Workflow AutomatisÃ© : Pipeline de donnÃ©es entiÃ¨rement planifiÃ©

QualitÃ© des DonnÃ©es : Validation Ã  chaque Ã©tape

Scalable : Traitement distribuÃ© des gros volumes

Reproductible : Composants dockerisÃ©s disponibles

##ğŸ“ˆ RÃ©sultats
###Traitement de 5+ millions d'enregistrements

-99.9% de cohÃ©rence des donnÃ©es

-RÃ©duction du temps de traitement de plusieurs heures Ã  quelques minutes

##ğŸ¤ Contributeurs
-Afdel Desmond KOMBOU

-Papa Yeriba NIANG

-SupervisÃ© par Mr. Patrick NGOUNE
```bash
