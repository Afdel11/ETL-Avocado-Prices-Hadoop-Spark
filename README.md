# Pipeline Big Data Avocado : Ingestion, Traitement et Analyse avec MySQL, NiFi, Spark et Hive

## Description 
ğŸ¥‘ Pipeline Big Data pour l'Analyse des Prix des Avocats
Ce projet dÃ©montre un workflow complet de traitement de donnÃ©es depuis l'ingestion initiale (fichiers CSV) jusqu'Ã  l'analyse avancÃ©e, en utilisant une stack Big Data moderne :

MySQL pour le stockage initial,

Apache NiFi pour l'automatisation des flux,

HDFS/Spark pour le traitement distribuÃ©,

Hive pour l'analyse SQL et la crÃ©ation de vues agrÃ©gÃ©es.

ğŸ”§ Stack Technique :
MySQL Â· Apache NiFi Â· HDFS Â· PySpark Â· Hive Â· Cron

### ğŸš€ FonctionnalitÃ©s ClÃ©s :

Ingestion flexible : Chargement de fichiers CSV dÃ©coupÃ©s dans MySQL, puis transfert vers HDFS via NiFi.

Traitement scalable :

Calcul du volume total par fichier avec Spark.

Enrichissement des donnÃ©es (extraction jour/mois depuis la date).

Analyse intelligente :

CrÃ©ation de tables/vues Hive pour requÃªter les donnÃ©es nettoyÃ©es.

AgrÃ©gation des volumes par mois (vue volume_per_month).

Automatisation :

Orchestration via cron (exÃ©cution pÃ©riodique des jobs Spark et NiFi).

### ğŸ“Š RÃ©sultats Concrets :

Optimisation du stockage : Architecture multi-couches (raw â†’ staging â†’ refined).

Monitoring : Logs et vÃ©rifications Ã  chaque Ã©tape pour garantir l'intÃ©gritÃ© des donnÃ©es.
