# ü•ë Pipeline Big Data pour l'Analyse des March√©s d'Avocats

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Spark 3.x](https://img.shields.io/badge/Spark-3.x-blue.svg)](https://spark.apache.org/)
[![Hive 3.x](https://img.shields.io/badge/Hive-3.x-orange.svg)](https://hive.apache.org/)

**Une solution compl√®te de traitement de donn√©es pour l'analyse du march√© des avocats utilisant l'√©cosyst√®me Hadoop**

![Architecture du Pipeline](https://via.placeholder.com/800x400?text=Diagramme+d'architecture) *(Remplacer par le vrai diagramme d'architecture)*

## üìå Aper√ßu du Projet
Ce projet impl√©mente une solution Big Data compl√®te pour traiter et analyser les donn√©es de march√© des avocats depuis des fichiers CSV jusqu'√† des insights actionnables. 

### Fonctionnalit√©s Principales
- **Traitement de gros volumes** (5+ millions d'enregistrements)
- **Pipeline enti√®rement automatis√©**
- **Qualit√© des donn√©es garantie** (99.9% de coh√©rence)
- **R√©duction drastique du temps de traitement** (de plusieurs heures √† quelques minutes)

## üõ†Ô∏è Stack Technique

| Composant        | Technologie Utilis√©e |
|------------------|----------------------|
| Ingestion        | Apache NiFi          |
| Stockage         | MySQL, HDFS          |
| Traitement       | PySpark              |
| Data Warehouse   | Hive                 |
| Orchestration    | Cron                 |
| Conteneurisation | Docker (optionnel)   |

## üìÇ √âtapes du Pipeline

### 1. Pr√©paration des Donn√©es
- D√©coupage du CSV source en partitions
- Chargement dans la base MySQL

### 2. Ingestion Automatis√©e
- Workflow NiFi extrait les donn√©es de MySQL
- Stockage des fichiers bruts dans HDFS (`/raw_avocado`)

### 3. Traitement des Donn√©es
- **Job Spark #1** : Calcule les m√©triques de volume ‚Üí Table Hive
- **Job Spark #2** : Enrichit avec des features de date ‚Üí Stockage HDFS raffin√© (`/refine_avocado`)

### 4. Analyse
- Tables externes Hive pour acc√®s SQL
- Vues agr√©g√©es (ex : `volume_per_month`)

### 5. Automatisation
- NiFi programm√© toutes les 5 minutes
- Jobs Spark planifi√©s via Cron

## üöÄ Guide de D√©marrage

### Pr√©requis
- Cluster Hadoop op√©rationnel
- MySQL 8.0+
- Apache NiFi
- Spark 3.x
- Hive 3.x

### Installation
```bash
# Cloner le d√©p√¥t
git clone https://github.com/Afdel11/ETL-Avocado-Prices-Hadoop-Spark.git

# Cr√©er les r√©pertoires HDFS
hdfs dfs -mkdir /raw_avocado
hdfs dfs -mkdir /staging_avocado
hdfs dfs -mkdir /refine_avocado

# Installer les d√©pendances
pip install -r requirements.txt
